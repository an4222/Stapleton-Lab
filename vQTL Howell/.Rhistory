b = b/b[lc]
print(rbind(a,b,c))
#####4-6.9#####
#####BigM#####
a = c(-1,3,2,4,10,0,10,0)
b = c(0,2,1,3,1,0,0,60)
c = c(0,-3,-3,-5,0,1,1,-120)
print(rbind(a,b,c))
#Preprocessing
a  = a -10*b -10*c
print(rbind(a,b,c))
#Iteration1
lc = which(a  == min(a[2:7]));lc
brat = b[length(b)]/b[lc]
crat = c[length(c)]/c[lc]
brat;crat
#####4-6.9#####
#####BigM#####
a = c(-1,3,2,4,10,0,10,0)
b = c(0,2,1,3,1,0,0,60)
c = c(0,3,3,5,0,-1,1,120)
print(rbind(a,b,c))
#Preprocessing
a  = a -10*b -10*c
print(rbind(a,b,c))
#Iteration1
lc = which(a  == min(a[2:7]));lc
brat = b[length(b)]/b[lc]
crat = c[length(c)]/c[lc]
brat;crat
af = a[lc]/b[lc]
cf = (c[lc])/(b[lc])
af;cf
a = a - af*b
c = c - cf*b
b = b/b[lc]
print(rbind(a,b,c))
#Iteration2
lc = which(a  == min(a[2:7]));lc
brat = b[length(b)]/b[lc]
crat = c[length(c)]/c[lc]
brat;crat
af = a[lc]/c[lc]
bf = b[lc]/c[lc]
af;bf
a = a - af*c
b = b - bf*c
c = c/c[lc]
print(rbind(a,b,c))
#####Phase I#####
a = c(-1,0,0,0,1,0,1,0)
b = c(0,2,1,3,1,0,0,60)
c = c(0,3,3,5,0,-1,1,120)
print(rbind(a,b,c))
#Preprocessing
a  = a -1*b -1*c
print(rbind(a,b,c))
#Iteration1
lc = which(a  == min(a[2:7]));lc
brat = b[length(b)]/b[lc]
crat = c[length(c)]/c[lc]
brat;crat
af = a[lc]/b[lc]
cf = (c[lc])/(b[lc])
af;cf
a = a - af*b
c = c - cf*b
b = b/b[lc]
print(rbind(a,b,c))
#Iteration2
lc = which(a  == min(a[2:7]));lc
brat = b[length(b)]/b[lc]
crat = c[length(c)]/c[lc]
brat;crat
af = a[lc]/c[lc]
bf = b[lc]/c[lc]
af;bf
a = a - af*c
b = b - bf*c
c = c/c[lc]
print(rbind(a,b,c))
pf(6.19,2,93, lower.tail = )
pf(6.19,2,93, lower.tail = F)
pf(11.8811,2,62, lower.tail = F)
dat = matrix(c(22,18,23,
12,16,13,
24,28,25,
17,11,18), ncol = 3, byrow = T)
dat
dat = c(22,18,23,
12,16,13,
24,28,25,
17,11,18)
?gl
hardness = data.frame(Treatment = gl(3,1,12), block = gl(4,3,12),dat)
hardness
dat = c(22,18,23,
12,16,13,
24,28,25,
17,11,18)
dat = data.frame(Treatment = gl(3,1,12), block = gl(4,3,12),dat)
dat = c(22,18,23,
12,16,13,
24,28,25,
17,11,18)
df = data.frame(Treatment = gl(3,1,12), block = gl(4,3,12),dat)
dat.aov = aov(dat~.,df)
summary(dat.aov)
mean(df)
mean(df[1,])
str(df)
mean(df$dat )
?pf
qf(.05,2,6, lower.tail = F)
qf(.05,3,6, lower.tail = F)
re = ((3*90.31) + (4*2*9.22))/(11*9.22)
re
TukeyHSD(dat.aov,conf.level=0.95)$Treatment
#####Analysis of NPV results#####
library("tidyverse")
library("randomForest")
library("car")
datas= read_csv(file = "C:/Users/Thomas/Desktop/Data/OrgNPVs.csv")
?randomForest
datasu = sapply(1:length(colnames(datas)), function(x){
dim(unique(datas[,x]))[1]
})
useless = which(datasu == 1)
datas = datas[,-useless]
keep = which(complete.cases(datas) == T)
datasc = datas[keep,]
#Visualizing the NPVs
hist(datasc$NPV,breaks = 100, main = "NPV Distribution", xlab = "NPVs")
# For some reason this doesn't work vector too big error
#modc = lm(NPV ~ ., datasc)
modc = lm(NPV ~ CreditScore + `MSA Code` + `MI Percentage` +
DTI + UPB + CLTV + LTV + `Interest Rate`  + `Original Term`, datasc)
plot(cooks.distance(modc))
outlc = which(cooks.distance(modc) >= (4/dim(datasc)[1]))
datascc = datasc[-outlc,]
modc1 = lm(NPV ~ CreditScore + `MSA Code` + `MI Percentage` +
DTI + UPB + CLTV + LTV + `Interest Rate`  + `Original Term`, datascc)
vif(modc1)
#It appears that there is high multicolinearity with CLTV and LTV, let's eliminate CLTV
modc2 = lm(NPV ~ CreditScore + DTI + UPB  +
LTV + `Interest Rate`  + `Original Term`, datascc)
vif(modc2)
summary(modc2)
AIC(modc2);BIC(modc2)
shapiro.test(datascc$NPV[1:5000])
hist(datascc$NPV,breaks = 50, main = "NPV Distribution", xlab = "NPVs")
npv  = datascc$NPV
n = length(datascc$NPV)
npv.ecdf<-ecdf(datascc$NPV)
acper<-npv.ecdf(npv)-1/2/n
avg.npv<-mean(npv);sd.npv<-sd(npv)
npv.tran<-qnorm(acper,avg.npv,sd.npv)
head(cbind(npv,acper,npv.tran))
shapiro.test(sample(npv.tran,5000))
par(mfrow = c(2,1))
hist(npv.tran, breaks = 50)
hist(npv, breaks = 50)
trandat = datascc
trandat$NPV = npv.tran
modc3 = lm(NPV ~ CreditScore + DTI + UPB  +
LTV + `Interest Rate`  + `Original Term`, trandat)
vif(modc3)
summary(modc3)
AIC(modc3);BIC(modc3)
mods = lm(NPV/UPB ~ CreditScore + DTI   +
LTV + `Interest Rate`  + `Original Term`, datascc)
summary(mods)
hist(datascc$NPV/datascc$UPB, breaks = 100)
mods = lm(NPV/UPB ~ CreditScore + DTI   +
LTV + `Interest Rate`  + `Original Term`, trandat)
summary(mods)
hist(trandat$NPV/trandat$UPB, breaks = 100)
dim(datascc)
#####Randomforest Exploratory#####
trainingset = sample(1:nrow(datascc),500)
train.rf = randomForest(NPV ~ ., CreditScore + DTI + UPB  + LTV + `Interest Rate`  + `Original Term`,
data = datascc, subset = trainingset)
colnames(datascc)
train.rf = randomForest(NPV ~ CreditScore + DTI + UPB  + LTV + `Interest Rate`  + `Original Term`,
data = datascc, subset = trainingset)
datascc$`Interest Rate`
all(!is.na(datascc$`Interest Rate`))
complete.cases(datascc)
all(complete.cases(datascc))
train.rf = randomForest(NPV ~ CreditScore + DTI + UPB  + LTV + `Interest Rate`  + `Original Term`,
data = datascc, subset = trainingset)
#####Randomforest Exploratory#####
names(datascc) = make.names(names(datascc))
names(datascc)
train.rf = randomForest(NPV ~ CreditScore + DTI + UPB  + LTV + Interest.Rate  + Original.Term,
data = datascc, subset = trainingset)
train.rf
trainingset = sample(1:nrow(datascc),5000)
train.rf = randomForest(NPV ~ CreditScore + DTI + UPB  + LTV + Interest.Rate  + Original.Term,
data = datascc, subset = trainingset)
train.rf
plot(train.rf)
par(mfrow)
par(mfrow = c(1,1))
plot(train.rf)
train.rf = randomForest(NPV ~.,
data = datascc, subset = trainingset)
names(datascc)
#####Randomforest Exploratory#####
badvars <- c(5,16,18,19,22)
rfdat = datascc[,-badvars]
#####Randomforest Exploratory#####
badvars <- c(5,16,18,19,22)
rfdat = datascc[,-badvars]
names(rfdat) = make.names(names(rfdat))
trainingset = sample(1:nrow(rfdat),5000)
train.rf = randomForest(NPV ~.,
data = rfdat, subset = trainingset)
names(rfdat)
sapply(1:ncol(rfdat), function(x){
unique(rfdat[x,])
})
sapply(1:ncol(rfdat), function(x){
length(unique(rfdat[x,]))
})
unq = sapply(1:ncol(rfdat), function(x){
unique(rfdat[x,])
})
unq
class(unq)
unq = sapply(1:ncol(rfdat), function(x){
unique(rfdat[,x])
})
unq
unq = sapply(1:ncol(rfdat), function(x){
length(unique(rfdat[,x]))
})
unq
unique(rfdat[,1])
unq = sapply(1:ncol(rfdat), function(x){
dim(unique(rfdat[,x]))[1]
})
unq
unq = sapply(1:ncol(rfdat), function(x){
c(dim(unique(rfdat[,x]))[1],
all(!is.na(rfdat[,x])))
})
unq
train.rf = randomForest(NPV ~.,
data = rfdat, subset = trainingset)
unq = sapply(1:ncol(rfdat), function(x){
c(dim(unique(rfdat[,x]))[1],
all(!is.na(rfdat[,x])),
all(!is.null(rfdat[,x])))
})
unq
unique(rfdat[,3])
unique(rfdat[,])
unique(rfdat[,14])
unique(rfdat[,7])
train.rf = randomForest(NPV ~.,
data = rfdat, subset = trainingset)
str(rfdat)
str(rfdat[,1])
class(rfdat[,1])
rfdat$Occupancy.Status = as.factor(rfdat$Occupancy.Status)
rfdat$Channel = as.factor(rfdat$Channel)
rfdat$PPM = as.factor(rfdat$PPM)
rfdat$Property.Type = as.factor(rfdat$Property.Type)
rfdat$Loan.Purpose = as.factor(rfdat$Loan.Purpose)
rfdat$Seller.Name = as.factor(rfdat$Seller.Name)
rfdat$Servicer.Name = as.factor(rfdat$Servicer.Name)
str(rfdat)
rfdat$FirstTimeHomebuyer = as.factor(rfdat$FirstTimeHomebuyer)
str(rfdat)
train.rf = randomForest(NPV ~.,
data = rfdat, subset = trainingset)
train.rf
plot(train.rf)
?double
oob.err=double(6)
test.err=double(6)
trainingset = sample(1:nrow(rfdat),500)
train.rf = randomForest(NPV ~.,
data = rfdat, subset = trainingset)
train.rf
plot(train.rf)
train = sample(1:nrow(rfdat),500)
train.rf = randomForest(NPV ~.,
data = rfdat, subset = train)
train.rf
plot(train.rf)
oob.err=double(6)
test.err=double(6)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:6)
{
rf=randomForest(NPV ~ . , data = rfdat , subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,rfdat[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(rfdat[-train,], mean( (NPV - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
test.err
oob.err
?matplot
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
train = sample(1:nrow(rfdat),5000)
train.rf = randomForest(NPV ~.,
data = rfdat, subset = train)
train.rf
plot(train.rf)
oob.err=double(6)
test.err=double(6)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:6)
{
rf=randomForest(NPV ~ . , data = rfdat , subset = train,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,rfdat[-train,]) #Predictions on Test Set for each Tree
test.err[mtry]= with(rfdat[-train,], mean( (NPV - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
test.err
oob.err
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
install.packages("RandomGLM")
install.packages("randomGLM")
####RGLM#####
library("randomGLM")
dim(rfdat)
rglm = randomGLM(x = rfdat[train,-20],y = rfdat$NPV[train])
class(rfdat$CreditScore)
class(rfdat$FirstPmt)
class(rfdat$FirstTimeHomebuyer)
?model.matrix
Prob2 = as.data.frame(matrix(rep(0,16*4), nrow = 16))
Prob2[,1] = 1:16
Prob2[,2] = c("C","D","A","B",
"B","C","D","A",
"A","B","C","D",
"D","A","B","C")
Prob2[,5] = c(10,14,7,8,
7,18,11,8,
5,10,11,9,
10,10,12,14)
colnames(Prob2) = c("OBS","Assembly.Method", "Operator", "Order","Time")
Prob2$Assembly.Method <- factor(Prob2$Assembly.Method)
Prob2$Operator<- factor(Prob2$Operator)
Prob2$Order <- factor(Prob2$Order)
Prob2
Prob2 = as.data.frame(matrix(rep(0,16*4), nrow = 16))
Prob2[,1] = 1:16
Prob2[,2] = c("C","D","A","B",
"B","C","D","A",
"A","B","C","D",
"D","A","B","C")
Prob2[,3] = gl(4,4)
Prob2[,4] = gl(4,1,16)
Prob2[,5] = c(10,14,7,8,
7,18,11,8,
5,10,11,9,
10,10,12,14)
Prob2
colnames(Prob2) = c("OBS","Assembly.Method", "Operator", "Order","Time")
Prob2
Prob2$Assembly.Method <- factor(Prob2$Assembly.Method)
Prob2
Prob2 = as.data.frame(matrix(rep(0,16*4), nrow = 16))
Prob2[,1] = 1:16
Prob2[,2] = c("C","D","A","B",
"B","C","D","A",
"A","B","C","D",
"D","A","B","C")
Prob2[,3] = gl(4,4)
Prob2[,4] = gl(4,1,16)
Prob2[,5] = c(10,14,7,8,
7,18,11,8,
5,10,11,9,
10,10,12,14)
colnames(Prob2) = c("OBS","Assembly.Method", "Operator", "Order","Time")
Prob2$Assembly.Method <- factor(Prob2$Assembly.Method)
Prob2
P2.fit <- lm(Time ~Assembly.Method + Operator + Order, data = Prob2)
anova(P2.fit)
which(Prob2$Assembly.Method == LETTERS[1])
sapply(LETTERS[1:4],function(x){
mean(Prob2$Time[which(Prob2$Assemby.Method == x)]) -
mean(as.numeric(Prob2$Time))
})
x = LETTERS[1]
which(Prob2$Assemby.Method == x)
x
which(Prob2$Assembly.Method == x)
Prob2$Time[which(Prob2$Assemby.Method == x)]
class(Prob2$Time)
Prob2$Time
mean(Prob2$Time)
sapply(LETTERS[1:4],function(x){
mean(Prob2$Time[which(Prob2$Assemby.Method == x)]) -
mean(Prob2$Time)
})
which(Prob2$Assemby.Method == x)
Prob2$Assembly.Method == "A"
which(Prob2$Assembly.Method == "A")
sapply(LETTERS[1:4],function(x){
mean(Prob2$Time[which(Prob2$Assemby.Method == eval(x))]) -
mean(Prob2$Time)
})
Prob2$Assemby.Method == x
which(Prob2$Assemby.Method == x)
sapply(LETTERS[1:4],function(x){
mean(Prob2$Time[which(Prob2$Assembly.Method == x)]) -
mean(Prob2$Time)
})
TukeyHSD(aov(P2.fit),conf.level=0.95)$Assembly.Method ;
plot(aov(P2.fit))
am<- data.frame(
yield=c(11,14,14,8,
8,12,10,12,
9,11,7,15,
9,8,18,6),
methods=factor(c("C","B","D","A",
"B","C","A","D",
"A","D","B","C",
"D","A","C","B")),
workplace=factor(c("beta", "gamma", "delta", "alpha",
"alpha", "delta", "gamma", "beta",
"delta", "alpha", "beta", "gamma",
"gamma", "beta", "alpha","delta")),
order=gl(4,4),
operator=gl(4,1,16))
sapply(am, class)
summary(am.lm <- lm(yield ~ ., data=am) )
anova(am.lm)
anova(fe.fit)
Prob1b = as.data.frame(matrix(rep(0,25*5), nrow = 25))
Prob1b[,1] = 1:25
Prob1b[,2] = c("A","B","C","D","E",
"B","C","D","E","A",
"C","D","E","A","B",
"D","E","A","B","C",
"E","A","B","C","D")
Prob1b[,3] = gl(5,5)
Prob1b[,4] = gl(5,1,25)
Prob1b[,5] = rep("MPG Reading",25)
colnames(Prob1b) = c("OBS","Blend", "Model", "Driver","Mpg")
Prob1b$Blend <- factor(Prob1b$Blend)
Prob1b$Model<- factor(Prob1b$Model)
Prob1b$Driver <- factor(Prob1b$Driver)
Prob1b
fe.fit <- lm(Mpg ~ Blend + Model + Driver, data = fe);
Prob1 <- tempfile()
cat(file=Prob1, "
1 D 1 1 15.5
2 B 1 2 33.9
3 C 1 3 13.2
4 A 1 4 29.1
5 B 2 1 16.3
6 C 2 2 26.6
7 A 2 3 19.4
8 D 2 4 22.8
9 C  3 1  10.8
10 A 3 2 31.1
11 D 3 3 17.1
12 B 3 4 30.3
13 A 4 1 14.7
14 D 4 2  34.0
15 B 4 3 19.7
16 C 4 4 21.6 ", sep=" ")
options(scipen=999) # suppressing scientific notation
fe <-read.table(Prob1, header=FALSE, col.names=c("OBS", "Blend", "Driver", "Model","Mpg"))
fe$Blend <- factor(fe$Blend)
fe$Model<- factor(fe$Model)
fe$Driver <- factor(fe$Driver)
fe.fit <- lm(Mpg ~ Blend + Model + Driver, data = fe);
anova(fe.fit)
#####Working with a howell Sampleset#####
#constructing the sample
library("tidyverse")
library("qtl")
library("vqtl")
setwd("C:/Users/Thomas/Documents/GitHub/Stapleton-Lab/vQTL Howell")
#####now to run the small scale analysis#####
crossobj = read.cross(format = "csv", file = "Howell-Cross-Object-Ratio.csv")
crossobj = drop.nullmarkers(crossobj)
crossobj <- calc.genoprob(crossobj)
##getting rid of redundant columns
tablec = read_csv("Howell-Cross-ObjectC2.csv")
unq = sapply(3:ncol(tablec),function(x){
print(x)
dim(unique(tablec[3:134,x]))[1]
})
length(which(unq == 3))
keep = which(unq == 3)
tablecc = tablec[,c(1,2,(keep+2))]
tablecc[1:2,1:2] = ""
ranc = sample(3:ncol(tablecc),500)
sapply(3:ncol(tablecc), function(x){
colnames(tablecc)[x] <<- paste("Marker",colnames(tablecc)[x], sep = "")
})
#####Adding a column for the ratio of Spliced/Unspliced#####
rat = sapply(3:dim(tablecc)[1],function(x){
as.numeric(tablecc[x,1])/
as.numeric(tablecc[x,2])
})
tablerat = cbind(c("","",rat),tablecc[,3:dim(tablecc)[2]])
colnames(tablerat)[1] <- "Ratio"
write_csv(tablerat, "Howell-Cross-Object-Ratio.csv")
